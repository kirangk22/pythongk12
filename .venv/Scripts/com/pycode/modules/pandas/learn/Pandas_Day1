
you can practice python pandas online here,
https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/rest/Prompting_REST.ipynb

https://github.com/KeithGalli/complete-pandas-tutorial.git
 C:\Users\admin\PycharmProjects\pythonProject2\.venv\Scripts\com\pycode\modules\pandas\complete-pandas-tutorial


https://colab.research.google.com:
you can open ipynb files.
File->opennotebook-->slect git repo--->goto folder to select ipynb file

# Formulas------------>
# bios.loc[#rows,#cols_list] here #rows,#cols_list both are optional params
# loc[some boolean condition] to Filter data *****IMP****
# loc[some boolean condition,[cols_list]] to Filter data *****IMP****
# use either boolean condition to Filter data or use slice operator to filter data based on index rows
#finally first param purpose is to Filter data

# here first param used to filter data rows based on condition,second param to grab/select cols in filtered data


#Assign values
df['col1']="my_value"
df['col_new']=df['col1'] * df['col2']
coffee.loc[0:2,['Units Sold']]=np.nan  # for 0to 2nd index rows assign null to column 'Units Sold'

# can check df has null values or not
coffee.info()  # info about dataframe. it shows total entries and non-null entries counts
print(coffee.isna().sum()) # shows each columns how many nulls there with counts



# filter using conditions
#df[(cond1) & (cond2)])
# here cond1=df["height_cm"]>215  & df["born_country"]=="USA"
# Ex: print(df[(df["height_cm"]>215) & (df["born_country"]=="USA")])
# See Ex: pds16_access_data_filter.py


# Filter rows based on Regex
# df["col"].str.contains(r'regex')

#df.query('')  usage for filter rows

# drop cols or rows
# Formauls :
# df.drop(index number of row)
# df.drop([index number of rows])
# df.drop(columns=[list of cols to drop])

# print(bios.info())  # to check Datatypes of columns

# pd.merge() to join dataframes..can do all joij using how=''
# pd.concat() to concat based on axis, union purposes axis 0 default..combines rows

# now check data above join bios.athlete_id with results.athlete_id  (joined col name same on both dfs)
# how= default inner, use whatever needed join
joined_df=pd.merge(results,bios,on='athlete_id',how='left')
#left_on='',right_on='' not needed as col name is same on both dfs
print(joined_df)
# after join if any duplicate columns are there,those comes with suffix x,y
# we can use our own suffix if needed.

# here apply() take entire row of dataframe
bios['athlet_ctg']=bios.apply(athlet_ctg,axis=1)
# axis=1 means rows.....passing one by one rows of dataframe
# axis=0 means cols in apply()



